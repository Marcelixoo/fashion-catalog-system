## Scalability & Uptime Guarantee Checklist

1. Elastic Scaling
   â€¢ ğŸ“¦ â€œEach customer can create their own index, and we scale horizontally across nodes using Kubernetes.â€
   â€¢ ğŸ§© â€œWe use index-per-tenant design, which simplifies isolation and scaling.â€

2. Sharding
   â€¢ ğŸ§± â€œLarge indexes are split into shards so we can parallelize indexing and querying.â€
   â€¢ ğŸ¯ â€œShard count is optimized per index size â€” small tenants get 1 shard, big ones may get 5+.â€

3. Replication
   â€¢ ğŸ” â€œEach shard has replicas for high availability. If one node fails, the replica takes over automatically.â€
   â€¢ ğŸ”’ â€œWe use 2 replicas for read redundancy and durability.â€

4. Distributed Infrastructure
   â€¢ ğŸŒ â€œIndexes and replicas are spread across multiple zones or nodes to reduce single points of failure.â€
   â€¢ ğŸ›° â€œWe use cloud-native scheduling (e.g. Kubernetes, StatefulSets) to balance load.â€

5. Fault Tolerance
   â€¢ âš™ï¸ â€œOur API layer uses health checks and timeouts. If a node is slow or down, we fail over or retry automatically.â€
   â€¢ ğŸ§¯ â€œWe maintain daily snapshots of all indexes to recover from critical failures.â€

6. Fast Recovery
   â€¢ ğŸ’¾ â€œWe can restore an index from backups in minutes if needed.â€
   â€¢ ğŸ”„ â€œRead queries are routed to replicas when primaries are rebalancing or restarting.â€

7. Performance for All Customers
   â€¢ ğŸš€ â€œFrequent queries and product lookups are cached at the edge or in Redis.â€
   â€¢ ğŸ“Š â€œWe monitor per-customer indexing and query load to adjust shard allocation dynamically.â€

â¸»

## Sharding euristics

ğŸ”§ What is a Shard?

A shard is a low-level unit of data and computation:
â€¢ Primary shard: holds the original data.
â€¢ Replica shard: copy of a primary shard used for failover and load balancing.

â¸»

âš–ï¸ Choosing Shard Count: Small vs. Large Indexes

âœ… Small Index (up to ~1M documents / <2 GB total size)
â€¢ Recommended shards:
ğŸ§± 1 primary, 1â€“2 replicas
â€¢ Why:
Over-sharding small indexes adds unnecessary overhead (too many file handles, memory usage).
â€¢ Example:
Boutique shop with 20k products â†’ 1 primary, 1 replica.
â€¢ Elasticsearch config:

"number_of_shards": 1,
"number_of_replicas": 1

â¸»

âš–ï¸ Medium Index (~1Mâ€“10M documents / 2â€“20 GB)
â€¢ Recommended shards:
ğŸ§± 2â€“3 primaries, 1â€“2 replicas
â€¢ Why:
Better parallelism for indexing and querying. One shard could become a bottleneck.
â€¢ Example:
Mid-sized marketplace with 5 million SKUs.

â¸»

ğŸš€ Large Index (>10M+ docs / >20 GB total size)
â€¢ Recommended shards:
ğŸ§± 5â€“10 primaries, depending on CPU, RAM, and query patterns
â• 1â€“2 replicas
â€¢ Why:
Large documents must be split to:
â€¢ Distribute load across nodes
â€¢ Parallelize indexing/querying
â€¢ Fit in RAM/heap comfortably per shard
â€¢ Example:
Enterprise catalog with 50M variants across thousands of products.

â¸»

ğŸ“Š Rule of Thumb for Elasticsearch
â€¢ â± Target shard size: ~10â€“30 GB per shard
â€¢ ğŸ§  Target shard count per node: ~20â€“30 shards per GB of heap (depends on usage)
â€¢ âœ… Shards too small â†’ overhead (CPU/memory); too large â†’ slower recovery and possible heap issues

â¸»

ğŸ§© Meilisearch Notes
â€¢ Meilisearch uses a single-shard design per index by default (no built-in sharding yet).
â€¢ To scale:
â€¢ Use index-per-tenant model (you already do this)
â€¢ Partition data at app layer (e.g. per category, region, brand)
â€¢ Horizontally scale Meilisearch nodes with load balancers or proxies (e.g. Meili Pro or your own sharding proxy)

â¸»

ğŸ§  Final Tip: Auto-Adjust Shards Based on Tenant Type

Tenant Type Document Count Recommended Shards Example Config
Hobby shop < 500k 1 primary, 1 replica Simple, fast access
Medium store 1Mâ€“5M 2â€“3 primaries Better query parallelism
Enterprise brand > 10M 5â€“10 primaries Load balancing + scalability

â¸»

## Sharding by vendor type

ğŸ“ Sizing Calculator (Rules of Thumb)

Input:
â€¢ Number of documents
â€¢ Average document size (JSON payload size)

â¸»

ğŸ“Š 1. Elasticsearch

Metric Estimate
Index size ~1.5â€“3Ã— avg doc size Ã— doc count
Shard count 1 shard per ~10â€“30 GB
Heap need ~1 GB heap per 20â€“30 shards
Field-heavy docs Consider 3Ã— inflation vs raw JSON
Replication overhead Multiply index size by (1 + replica count)

ğŸ“Œ Example
â€¢ 5M products Ã— 3 KB â†’ ~15 GB raw
â€¢ ES index size: ~30â€“45 GB
â€¢ Shards: 2â€“4 primaries
â€¢ Heap: 2â€“4 GB minimum (with ~1 GB for each 25 shards)

â¸»

âš¡ 2. Meilisearch

Metric Estimate
Index size ~2â€“4Ã— raw JSON (due to full-text + facets)
Max index size ~10â€“20M docs per node (recommended)
Memory usage ~1â€“2Ã— index size in RAM (everything is mmapâ€™d)
Sharding support âŒ No native sharding â€” you shard at app level

ğŸ“Œ Example
â€¢ 2M products Ã— 3 KB â†’ 6 GB raw â†’ 12â€“15 GB Meili index
â€¢ RAM usage: 15â€“25 GB
â€¢ Shard manually via:
â€¢ Separate nodes per index (e.g. DE vs FR)
â€¢ Proxy and merge at app layer

â¸»

â˜€ï¸ 3. Apache Solr

Metric Estimate
Index size ~1.5â€“2.5Ã— raw JSON
Shard count Similar to ES (~10â€“30 GB/shard)
Heap usage ~1â€“1.5 GB per 10M docs for basic queries
Replicas Use SolrCloud for HA

ğŸ“Œ Example
â€¢ 5M Ã— 3 KB = 15 GB raw â†’ ~30 GB Solr index
â€¢ Heap: 2â€“4 GB for indexing + search (adjust JVM -Xmx)
â€¢ Shards: 2â€“3 primaries, 1â€“2 replicas

â¸»

ğŸ§® Rough Sizing Calculator (Pseudocode)

func EstimateShardSize(docCount int, avgDocKB int) (esIndexSizeGB, meiliIndexSizeGB, solrIndexSizeGB float64) {
rawSizeGB := float64(docCount*avgDocKB) / 1024.0 / 1024.0
esIndexSizeGB = rawSizeGB * 2.5
meiliIndexSizeGB = rawSizeGB _ 3.0
solrIndexSizeGB = rawSizeGB _ 2.0
return
}

â¸»

âœ… Summary Table

Engine Inflated Index Size Shardable? Memory Footprint When to Choose
Elasticsearch ~2â€“3Ã— âœ… Native ~1 GB/25 shards Large, filtered/faceted search
Meilisearch ~2â€“4Ã— âŒ Manual RAM â‰ˆ Index Size Lightweight, instant search
Solr ~1.5â€“2.5Ã— âœ… Native JVM heap ~1.5 GB/10M Powerful but more ops-heavy
